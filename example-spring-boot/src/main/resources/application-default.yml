spring:
  kafka:
    bootstrap-servers: localhost:9092
    listener:
      # ack模式，当enable-auto-commit设置为false时，表示kafka的offset手动维护，手动调用Acknowledgment.acknowledge()后立即提交
      ack-mode: MANUAL_IMMEDIATE
      # 监听线程数
      concurrency: 30
      # 消费者监听的topic不存在时，项目会报错，设置为false
      missing-topics-fatal: false
      # 消费模式，batch：批量；single：单条
      type: single
    producer:
      acks: 1
      # 写入失败时，重试次数
      retries: 3
      # 每次批量发送消息的数量，produce积累到一定数据，一次发送
      batch-size: 1200
      # produce积累数据一次发送，缓存大小达到buffer.memory就发送数据
      buffer-memory: 33554432
      # 指定消息key和消息体的编解码方式
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
    consumer:
      # 消费组名称
      group-id: my-test-group
      # 是否自动提交
      enable-auto-commit: false
      # 自动提交的时间间隔
      auto-commit-interval: 1000
      #  offset的消费位置
      auto-offset-reset: latest
      # 最大拉取条数（一次poll最多返回的记录数）
      max-poll-records: 100
      # 心跳时间
      heartbeat-interval: 3000
      # 指定消息key和消息体的编解码方式
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer

  # datasource config
  datasource:
    url: jdbc:mysql://127.0.0.1:3306/debezium?useUnicode=true&characterEncoding=utf8&allowPublicKeyRetrieval=true&useSSL=false&serverTimezone=Asia/Shanghai
    driver-class-name: com.mysql.cj.jdbc.Driver
    username: root
    password: 123456
    type: com.alibaba.druid.pool.DruidDataSource
    druid:
      aop-patterns: com.bruce.debezium.*
      initial-size: 5
      min-idle: 10
      max-active: 20
      max-wait: 60000
      time-between-eviction-runs-millis: 60000
      min-evictable-idle-time-millis: 300000
      validation-query: SELECT 1 FROM DUAL
      test-while-idle: true
      test-on-borrow: false
      test-on-return: false
      pool-prepared-statements: true
      max-pool-prepared-statement-per-connection-size: 20
      # connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=1000
      # filters: stat,wall
      # 配置监控统计拦截的 Filter，去掉后监控界面 SQL 无法统计，wall 用于防火墙
      filter:
        stat:
          slow-sql-millis: 1500 #设置所有超过1500ms的查询都是慢查询
          logSlowSql: true
          enabled: true
          merge-sql: true
        wall: #开启防火墙的功能
          enabled: true
          config:
          drop-table-allow: false
      # 配置 DruidStatFilter
      web-stat-filter:
        enabled: true
        url-pattern: /*
        exclusions: .js,*.gif,*.jpg,*.bmp,*.png,*.css,*.ico,/druid/*
        # 配置 DruidStatViewServlet
      stat-view-servlet:
        url-pattern: /druid/*
        # IP 白名单，没有配置或者为空，则允许所有访问
        allow: 127.0.0.1
        # IP 黑名单，若白名单也存在，则优先使用
        deny: 192.168.31.253
        # 禁用 HTML 中 Reset All 按钮
        reset-enable: false
        # 登录用户名/密码
        login-username: admin
        login-password: 123456
        enabled: true

  # mybatis-plus
mybatis-plus:
  mapper-locations: classpath:mapper/*.xml
  global-config:
    db-config:
      #主键类型  auto:数据库ID自增 1:"用户输入ID",2:"全局唯一ID (数字类型唯一ID)", 3:"全局唯一ID UUID";
      id-type: auto
      #字段策略 IGNORED:"忽略判断"  NOT_NULL:"非 NULL 判断")  NOT_EMPTY:"非空判断"
      field-strategy: NOT_NULL
      #数据库类型
      db-type: MYSQL
      #逻辑删除字段
      logic-delete-field: is_deleted
      #删除值
      logic-delete-value: 1
      #非删除值
      logic-not-delete-value: 0
  configuration:
    # 是否开启自动驼峰命名规则映射:从数据库列名到Java属性驼峰命名的类似映射
    map-underscore-to-camel-case: true
    # 如果查询结果中包含空值的列，则 MyBatis 在映射的时候，不会映射这个字段
    call-setters-on-nulls: true
    # 这个配置会将执行的sql打印出来，在开发或测试的时候可以用
    # log-impl: org.apache.ibatis.logging.stdout.StdOutImpl

kafka:
  enable: false
  topics:
    my_test_topic: my-test-topic
  groups:
    my_test_group: my-test-group

